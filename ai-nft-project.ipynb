{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5892a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:09:48.177526Z",
     "iopub.status.busy": "2025-09-02T17:09:48.177261Z",
     "iopub.status.idle": "2025-09-02T17:09:48.184961Z",
     "shell.execute_reply": "2025-09-02T17:09:48.184200Z"
    },
    "papermill": {
     "duration": 0.012582,
     "end_time": "2025-09-02T17:09:48.186119",
     "exception": false,
     "start_time": "2025-09-02T17:09:48.173537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indian-monuments-image-dataset', 'snake-dataset-india', 'indian-paintings-dataset', 'top-500-indian-cities', 'indian-signboard-image-dataset', 'indian-currency-notes-classifier', 'postage-stamp-data-set', 'indian-actor-images-dataset', 'indian-food-images-dataset', 'indian-currency-note-images-dataset-2020', 'indian-cricketers-images', 'indian-classical-musical-instruments', 'bird-species-classification', 'india-famous-personalities-image-dataset', 'indian-dance-images', '200-bird-species-with-11788-images']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The default path for Kaggle input files\n",
    "input_path = '/kaggle/input/'\n",
    "\n",
    "# List all items in the input directory and filter for folders\n",
    "folders = [folder for folder in os.listdir(input_path) \n",
    "           if os.path.isdir(os.path.join(input_path, folder))]\n",
    "\n",
    "# Print the list of folders\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d92d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:09:48.191493Z",
     "iopub.status.busy": "2025-09-02T17:09:48.191280Z",
     "iopub.status.idle": "2025-09-02T17:13:19.463459Z",
     "shell.execute_reply": "2025-09-02T17:13:19.462685Z"
    },
    "papermill": {
     "duration": 211.276212,
     "end_time": "2025-09-02T17:13:19.464767",
     "exception": false,
     "start_time": "2025-09-02T17:09:48.188555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the dataset combination process...\n",
      "Destination folder created at: /kaggle/working/combined_indian_dataset/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folders: 100%|██████████| 10/10 [03:31<00:00, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "✅ Dataset combination complete!\n",
      "Total images copied: 21813\n",
      "Your new dataset is ready at: /kaggle/working/combined_indian_dataset/\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. Define the base path for Kaggle input\n",
    "BASE_INPUT_PATH = '/kaggle/input/'\n",
    "\n",
    "# 2. List of the source folders you want to combine\n",
    "SOURCE_FOLDERS = [\n",
    "    'india-famous-personalities-image-dataset', 'indian-classical-musical-instruments',\n",
    "     'indian-dance-images',\n",
    "    'indian-monuments-image-dataset', 'indian-currency-note-images-dataset-2020',\n",
    "    'postage-stamp-data-set',\n",
    "    'indian-currency-notes-classifier', 'indian-food-images-dataset', 'top-500-indian-cities', 'indian-paintings-dataset'\n",
    "]\n",
    "\n",
    "# 3. Define the destination for your new, combined dataset\n",
    "#    Kaggle allows you to write to the '/kaggle/working/' directory\n",
    "DESTINATION_PATH = '/kaggle/working/combined_indian_dataset/'\n",
    "\n",
    "# --- Main Script ---\n",
    "print(f\"Starting the dataset combination process...\")\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(DESTINATION_PATH, exist_ok=True)\n",
    "print(f\"Destination folder created at: {DESTINATION_PATH}\")\n",
    "\n",
    "image_counter = 0\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "\n",
    "# Loop through each source folder\n",
    "for folder_name in tqdm(SOURCE_FOLDERS, desc=\"Processing Folders\"):\n",
    "    source_path = os.path.join(BASE_INPUT_PATH, folder_name)\n",
    "    \n",
    "    # Check if the source directory actually exists\n",
    "    if not os.path.isdir(source_path):\n",
    "        print(f\"Warning: Folder '{folder_name}' not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # os.walk() recursively finds all files in all subdirectories\n",
    "    for dirpath, _, filenames in os.walk(source_path):\n",
    "        for filename in filenames:\n",
    "            # Check if the file is an image\n",
    "            if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "                \n",
    "                # Construct the full path of the source image\n",
    "                source_file_path = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # Get the original file extension (e.g., '.jpg')\n",
    "                original_extension = os.path.splitext(filename)[1]\n",
    "                \n",
    "                # Create a new, unique filename to avoid overwrites\n",
    "                new_filename = f\"image_{image_counter:06d}{original_extension}\"\n",
    "                destination_file_path = os.path.join(DESTINATION_PATH, new_filename)\n",
    "                \n",
    "                # Copy the file to the new destination\n",
    "                shutil.copy2(source_file_path, destination_file_path)\n",
    "                \n",
    "                # Increment the counter\n",
    "                image_counter += 1\n",
    "\n",
    "print(\"\\n-------------------------------------------------\")\n",
    "print(\"✅ Dataset combination complete!\")\n",
    "print(f\"Total images copied: {image_counter}\")\n",
    "print(f\"Your new dataset is ready at: {DESTINATION_PATH}\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96e90fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:13:19.471114Z",
     "iopub.status.busy": "2025-09-02T17:13:19.470903Z",
     "iopub.status.idle": "2025-09-02T17:13:20.184725Z",
     "shell.execute_reply": "2025-09-02T17:13:20.183689Z"
    },
    "papermill": {
     "duration": 0.718297,
     "end_time": "2025-09-02T17:13:20.185901",
     "exception": false,
     "start_time": "2025-09-02T17:13:19.467604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sub-folder at: /kaggle/working/combined_indian_dataset/images\n",
      "Found 21813 images to move...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organizing files: 100%|██████████| 21813/21813 [00:00<00:00, 36705.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All files have been moved successfully!\n",
      "You can now re-run your training script.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The path to your combined dataset\n",
    "base_path = '/kaggle/working/combined_indian_dataset/'\n",
    "\n",
    "# The path for the new sub-folder that ImageFolder expects\n",
    "new_class_folder_path = os.path.join(base_path, 'images')\n",
    "\n",
    "# Create the new sub-folder\n",
    "print(f\"Creating sub-folder at: {new_class_folder_path}\")\n",
    "os.makedirs(new_class_folder_path, exist_ok=True)\n",
    "\n",
    "# Find all files in the base directory\n",
    "files_to_move = [f for f in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, f))]\n",
    "\n",
    "print(f\"Found {len(files_to_move)} images to move...\")\n",
    "\n",
    "# Move each file into the new sub-folder\n",
    "for filename in tqdm(files_to_move, desc=\"Organizing files\"):\n",
    "    source = os.path.join(base_path, filename)\n",
    "    destination = os.path.join(new_class_folder_path, filename)\n",
    "    shutil.move(source, destination)\n",
    "\n",
    "print(\"\\n✅ All files have been moved successfully!\")\n",
    "print(\"You can now re-run your training script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9210e74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:13:20.193386Z",
     "iopub.status.busy": "2025-09-02T17:13:20.193166Z",
     "iopub.status.idle": "2025-09-02T18:20:56.529044Z",
     "shell.execute_reply": "2025-09-02T18:20:56.528028Z"
    },
    "papermill": {
     "duration": 4056.341335,
     "end_time": "2025-09-02T18:20:56.530498",
     "exception": false,
     "start_time": "2025-09-02T17:13:20.189163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1740485352.py:195: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n",
      "Epoch 1/15:   0%|          | 0/2725 [00:00<?, ?it/s]/tmp/ipykernel_19/1740485352.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n",
      "Epoch 1/15:   5%|▍         | 126/2725 [00:13<03:46, 11.49it/s, Loss=1.0085, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1/15:  64%|██████▎   | 1731/2725 [02:47<01:28, 11.23it/s, Loss=0.7892, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1/15: 100%|██████████| 2725/2725 [04:28<00:00, 10.14it/s, Loss=0.7645, LR=2.0e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Loss: 0.8485\n",
      "Saved checkpoint to checkpoint_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:  12%|█▏        | 331/2725 [00:32<03:37, 11.00it/s, Loss=0.6950, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 2/15:  69%|██████▉   | 1881/2725 [03:06<01:29,  9.42it/s, Loss=0.6842, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 2/15: 100%|██████████| 2725/2725 [04:29<00:00, 10.13it/s, Loss=0.5801, LR=2.0e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Average Loss: 0.6426\n",
      "Saved checkpoint to checkpoint_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:   7%|▋         | 180/2725 [00:18<03:59, 10.61it/s, Loss=0.5139, LR=1.9e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 3/15:  17%|█▋        | 460/2725 [00:47<03:29, 10.83it/s, Loss=0.5693, LR=1.9e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 3/15: 100%|██████████| 2725/2725 [04:09<00:00, 10.91it/s, Loss=0.5132, LR=1.9e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Average Loss: 0.5458\n",
      "Saved checkpoint to checkpoint_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:  16%|█▋        | 445/2725 [00:44<02:23, 15.85it/s, Loss=0.4406, LR=1.8e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 4/15:  66%|██████▌   | 1794/2725 [02:53<01:25, 10.90it/s, Loss=0.4700, LR=1.8e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 4/15: 100%|██████████| 2725/2725 [04:26<00:00, 10.24it/s, Loss=0.4231, LR=1.8e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Average Loss: 0.4813\n",
      "Saved checkpoint to checkpoint_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:  29%|██▉       | 795/2725 [01:18<02:27, 13.09it/s, Loss=0.5009, LR=1.7e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 5/15:  37%|███▋      | 1005/2725 [01:39<02:22, 12.11it/s, Loss=0.5370, LR=1.7e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 5/15: 100%|██████████| 2725/2725 [04:27<00:00, 10.20it/s, Loss=0.4289, LR=1.7e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished. Average Loss: 0.4360\n",
      "Saved checkpoint to checkpoint_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:  11%|█         | 289/2725 [00:28<03:05, 13.12it/s, Loss=0.4414, LR=1.5e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 6/15:  31%|███       | 835/2725 [01:22<02:26, 12.90it/s, Loss=0.5198, LR=1.5e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 6/15: 100%|██████████| 2725/2725 [04:35<00:00,  9.88it/s, Loss=0.2805, LR=1.5e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished. Average Loss: 0.4049\n",
      "Saved checkpoint to checkpoint_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:   7%|▋         | 185/2725 [00:17<03:48, 11.10it/s, Loss=0.4389, LR=1.3e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 7/15:  26%|██▋       | 716/2725 [01:09<02:28, 13.52it/s, Loss=0.4699, LR=1.3e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 7/15: 100%|██████████| 2725/2725 [04:29<00:00, 10.13it/s, Loss=0.3865, LR=1.3e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished. Average Loss: 0.3778\n",
      "Saved checkpoint to checkpoint_epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:  16%|█▌        | 435/2725 [00:45<03:10, 12.02it/s, Loss=0.3634, LR=1.1e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 8/15:  23%|██▎       | 622/2725 [01:03<02:39, 13.22it/s, Loss=0.3579, LR=1.1e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 8/15: 100%|██████████| 2725/2725 [04:29<00:00, 10.10it/s, Loss=0.2440, LR=1.1e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished. Average Loss: 0.3544\n",
      "Saved checkpoint to checkpoint_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:  14%|█▍        | 386/2725 [00:40<04:01,  9.70it/s, Loss=0.2933, LR=9.0e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 9/15:  15%|█▌        | 409/2725 [00:43<04:28,  8.61it/s, Loss=0.2612, LR=9.0e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 9/15: 100%|██████████| 2725/2725 [04:31<00:00, 10.05it/s, Loss=0.2563, LR=9.0e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished. Average Loss: 0.3380\n",
      "Saved checkpoint to checkpoint_epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:  42%|████▏     | 1135/2725 [01:51<02:13, 11.89it/s, Loss=0.3919, LR=6.9e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 10/15:  46%|████▋     | 1266/2725 [02:03<01:46, 13.74it/s, Loss=0.2540, LR=6.9e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 10/15: 100%|██████████| 2725/2725 [04:28<00:00, 10.16it/s, Loss=0.3077, LR=6.9e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished. Average Loss: 0.3256\n",
      "Saved checkpoint to checkpoint_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:  10%|█         | 273/2725 [00:28<04:05, 10.00it/s, Loss=0.2944, LR=5.0e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 11/15:  15%|█▍        | 407/2725 [00:43<04:22,  8.82it/s, Loss=0.4735, LR=5.0e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 11/15: 100%|██████████| 2725/2725 [04:27<00:00, 10.20it/s, Loss=0.3578, LR=5.0e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 finished. Average Loss: 0.3135\n",
      "Saved checkpoint to checkpoint_epoch_11.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:   4%|▍         | 119/2725 [00:12<03:38, 11.95it/s, Loss=0.3100, LR=3.3e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 12/15:  36%|███▌      | 978/2725 [01:35<03:45,  7.74it/s, Loss=0.2057, LR=3.3e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 12/15: 100%|██████████| 2725/2725 [04:26<00:00, 10.22it/s, Loss=0.4096, LR=3.3e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 finished. Average Loss: 0.3116\n",
      "Saved checkpoint to checkpoint_epoch_12.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:   3%|▎         | 88/2725 [00:08<03:31, 12.46it/s, Loss=0.2204, LR=1.9e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 13/15:  17%|█▋        | 456/2725 [00:45<03:34, 10.57it/s, Loss=0.3863, LR=1.9e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 13/15: 100%|██████████| 2725/2725 [04:31<00:00, 10.05it/s, Loss=0.1984, LR=1.9e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 finished. Average Loss: 0.3059\n",
      "Saved checkpoint to checkpoint_epoch_13.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15:  21%|██▏       | 585/2725 [01:00<03:40,  9.69it/s, Loss=0.3333, LR=8.6e-07]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 14/15:  29%|██▊       | 778/2725 [01:19<03:38,  8.92it/s, Loss=0.2343, LR=8.6e-07]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 14/15: 100%|██████████| 2725/2725 [04:30<00:00, 10.09it/s, Loss=0.1993, LR=8.6e-07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 finished. Average Loss: 0.3047\n",
      "Saved checkpoint to checkpoint_epoch_14.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15:  14%|█▍        | 381/2725 [00:39<03:25, 11.40it/s, Loss=0.3350, LR=2.2e-07]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 15/15:  21%|██        | 570/2725 [00:57<02:34, 13.99it/s, Loss=0.4707, LR=2.2e-07]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 15/15: 100%|██████████| 2725/2725 [04:31<00:00, 10.05it/s, Loss=0.3200, LR=2.2e-07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 finished. Average Loss: 0.3027\n",
      "Saved checkpoint to checkpoint_epoch_15.pth\n",
      "Training finished. Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [00:08<00:00, 117.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final generated images to 'final_generated_sample.png'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "# ======================================================================================\n",
    "# PART 1: THE VAE ARCHITECTURE (THE COMPRESSOR) - (No changes needed here)\n",
    "# ======================================================================================\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        modules = []\n",
    "        hidden_dims = [32, 64, 128, 256]\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*16, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*16, latent_dim)\n",
    "\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 16)\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3, stride=2,\n",
    "                                       padding=1, output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        \n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3, stride=2,\n",
    "                                               padding=1, output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels=3,\n",
    "                                      kernel_size=3, padding=1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, x):\n",
    "        spatial_latent = self.encoder(x)\n",
    "        result = torch.flatten(spatial_latent, start_dim=1)\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        return mu, log_var, spatial_latent\n",
    "\n",
    "    def decode(self, z):\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 256, 4, 4)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var, _ = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mu, log_var\n",
    "\n",
    "# ======================================================================================\n",
    "# PART 2: THE U-NET ARCHITECTURE (THE DENOISER) - (No changes needed here)\n",
    "# ======================================================================================\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        half_dim = self.n_channels // 2\n",
    "        exponents = torch.arange(half_dim, device=t.device).float() / (half_dim - 1)\n",
    "        embeddings = torch.exp(-math.log(10000) * exponents)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        return torch.cat([embeddings.sin(), embeddings.cos()], dim=-1)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.act1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(32, out_channels)\n",
    "        self.act2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        time_emb_proj = self.time_emb(self.act2(t))\n",
    "        h = h + time_emb_proj[:, :, None, None]\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(32, n_channels)\n",
    "        self.qkv = nn.Conv2d(n_channels, n_channels * 3, kernel_size=1)\n",
    "        self.proj_out = nn.Conv2d(n_channels, n_channels, kernel_size=1)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        h_ = self.norm(x)\n",
    "        qkv = self.qkv(h_)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        q = q.view(b, c, h * w); k = k.view(b, c, h * w); v = v.view(b, c, h * w)\n",
    "        attn = torch.einsum('bci,bcj->bij', q, k) * (c ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = torch.einsum('bij,bcj->bci', attn, v)\n",
    "        out = out.view(b, c, h, w)\n",
    "        return x + self.proj_out(out)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=256, out_channels=256, n_channels=320):\n",
    "        super().__init__()\n",
    "        time_emb_dim = n_channels * 4\n",
    "        self.time_embedding = TimeEmbedding(time_emb_dim)\n",
    "        self.inc = nn.Conv2d(in_channels, n_channels, kernel_size=3, padding=1)\n",
    "        self.down1_res = ResidualBlock(n_channels, n_channels, time_emb_dim)\n",
    "        self.down1_attn = AttentionBlock(n_channels)\n",
    "        self.down2_conv = nn.Conv2d(n_channels, n_channels * 2, kernel_size=3, stride=2, padding=1)\n",
    "        self.down2_res = ResidualBlock(n_channels * 2, n_channels * 2, time_emb_dim)\n",
    "        self.down2_attn = AttentionBlock(n_channels * 2)\n",
    "        self.bot_res1 = ResidualBlock(n_channels * 2, n_channels * 2, time_emb_dim)\n",
    "        self.bot_attn = AttentionBlock(n_channels * 2)\n",
    "        self.bot_res2 = ResidualBlock(n_channels * 2, n_channels * 2, time_emb_dim)\n",
    "        self.up1_res = ResidualBlock(n_channels * 4, n_channels * 2, time_emb_dim)\n",
    "        self.up1_attn = AttentionBlock(n_channels * 2)\n",
    "        self.up1_conv_transpose = nn.ConvTranspose2d(n_channels * 2, n_channels, kernel_size=2, stride=2)\n",
    "        self.up2_res = ResidualBlock(n_channels * 2, n_channels, time_emb_dim)\n",
    "        self.up2_attn = AttentionBlock(n_channels)\n",
    "        self.outc = nn.Sequential(nn.GroupNorm(32, n_channels), nn.SiLU(), nn.Conv2d(n_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        t_emb = self.time_embedding(t)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1_res(x1, t_emb); x2 = self.down1_attn(x2)\n",
    "        x3_conv = self.down2_conv(x2)\n",
    "        x3 = self.down2_res(x3_conv, t_emb); x3 = self.down2_attn(x3)\n",
    "        x_bot = self.bot_res1(x3, t_emb); x_bot = self.bot_attn(x_bot); x_bot = self.bot_res2(x_bot, t_emb)\n",
    "        x = torch.cat([x_bot, x3], dim=1)\n",
    "        x = self.up1_res(x, t_emb); x = self.up1_attn(x); x = self.up1_conv_transpose(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up2_res(x, t_emb); x = self.up2_attn(x)\n",
    "        return self.outc(x)\n",
    "\n",
    "# ======================================================================================\n",
    "# PART 3: THE DRIVER / CONTROLLER (THE LOGIC) - (Major changes here)\n",
    "# ======================================================================================\n",
    "class DiffusionTrainer:\n",
    "    ### CHANGE: Added epochs to __init__ to configure the scheduler\n",
    "    def __init__(self, unet_model, vae_model, timesteps=1000, device='cpu', lr=1e-4, epochs=50):\n",
    "        self.device = device\n",
    "        self.unet_model = unet_model.to(device)\n",
    "        self.vae_model = vae_model.to(device)\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        for param in self.vae_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.betas = self._linear_beta_schedule(timesteps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        \n",
    "        ### CHANGE: Switched to AdamW, a better optimizer\n",
    "        self.optimizer = torch.optim.AdamW(self.unet_model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        ### CHANGE: Added a learning rate scheduler for better convergence\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
    "\n",
    "        ### CHANGE: Added a gradient scaler for automatic mixed precision (faster training)\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n",
    "\n",
    "\n",
    "    def _linear_beta_schedule(self, timesteps):\n",
    "        beta_start = 0.0001\n",
    "        beta_end = 0.02\n",
    "        return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "    def _get_index_from_list(self, vals, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = vals.gather(-1, t) \n",
    "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Use autocast for mixed precision to speed up training on GPU\n",
    "        with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n",
    "            _, _, x_0 = self.vae_model.encode(real_images)\n",
    "            t = torch.randint(0, self.timesteps, (x_0.shape[0],), device=self.device).long()\n",
    "            noise = torch.randn_like(x_0)\n",
    "            \n",
    "            alphas_cumprod_t = self._get_index_from_list(self.alphas_cumprod, t, x_0.shape)\n",
    "            \n",
    "            noisy_latent = torch.sqrt(alphas_cumprod_t) * x_0 + torch.sqrt(1. - alphas_cumprod_t) * noise\n",
    "            predicted_noise = self.unet_model(noisy_latent, t)\n",
    "            loss = self.criterion(noise, predicted_noise)\n",
    "\n",
    "        ### CHANGE: Use the scaler for the backward pass\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_images=4):\n",
    "        latent_x = torch.randn((num_images, 256, 4, 4), device=self.device)\n",
    "        \n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc=\"Sampling\", total=self.timesteps):\n",
    "            t = torch.full((num_images,), i, device=self.device, dtype=torch.long)\n",
    "            predicted_noise = self.unet_model(latent_x, t)\n",
    "            alpha_t = self._get_index_from_list(self.alphas, t, latent_x.shape)\n",
    "            alphas_cumprod_t = self._get_index_from_list(self.alphas_cumprod, t, latent_x.shape)\n",
    "            beta_t = self._get_index_from_list(self.betas, t, latent_x.shape)\n",
    "            noise_term = ((1 - alpha_t) / torch.sqrt(1 - alphas_cumprod_t)) * predicted_noise\n",
    "            latent_x = (1 / torch.sqrt(alpha_t)) * (latent_x - noise_term)\n",
    "            if i > 0:\n",
    "                z = torch.randn_like(latent_x)\n",
    "                latent_x += torch.sqrt(beta_t) * z\n",
    "\n",
    "        sampled_images = self.vae_model.decoder(latent_x)\n",
    "        sampled_images = self.vae_model.final_layer(sampled_images)\n",
    "        return sampled_images\n",
    "\n",
    "# ======================================================================================\n",
    "# PART 4: MAIN EXECUTION BLOCK (PUTTING IT ALL TOGETHER)\n",
    "# ======================================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE = 8\n",
    "    IMG_SIZE = 64\n",
    "    ### CHANGE: Reduced epochs as requested. With faster training, 25 might be enough.\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 2e-5 ### CHANGE: Slightly increased learning rate, good starting point with a scheduler\n",
    "    DATASET_PATH = \"/kaggle/working/combined_indian_dataset/\"\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Data Loading and Transformations ---\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = torchvision.datasets.ImageFolder(root=DATASET_PATH, transform=transforms)\n",
    "    \n",
    "    ### CHANGE: Reduced num_workers to prevent CPU bottleneck and added pin_memory for speed\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # --- Initialize Models ---\n",
    "    vae = VAE(in_channels=3, latent_dim=128)\n",
    "    # You should load your pre-trained VAE weights here if you have them\n",
    "    # vae.load_state_dict(torch.load('path/to/your/vae.pth'))\n",
    "    \n",
    "    VAE_ENCODER_OUTPUT_CHANNELS = 256 \n",
    "    unet = UNet(in_channels=VAE_ENCODER_OUTPUT_CHANNELS, out_channels=VAE_ENCODER_OUTPUT_CHANNELS)\n",
    "    \n",
    "    # --- Initialize the Driver/Controller ---\n",
    "    trainer = DiffusionTrainer(unet, vae, timesteps=1000, device=DEVICE, lr=LEARNING_RATE, epochs=EPOCHS)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for batch_idx, (real_images, _) in enumerate(progress_bar):\n",
    "            real_images = real_images.to(DEVICE)\n",
    "            loss = trainer.train_step(real_images)\n",
    "            total_loss += loss\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss:.4f}\", \"LR\": f\"{trainer.scheduler.get_last_lr()[0]:.1e}\"})\n",
    "        \n",
    "        ### CHANGE: Step the scheduler at the end of each epoch\n",
    "        trainer.scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        ### CHANGE: Save a comprehensive checkpoint instead of just the weights\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'unet_state_dict': trainer.unet_model.state_dict(),\n",
    "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': trainer.scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        print(f\"Saved checkpoint to checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # --- Sampling ---\n",
    "    print(\"Training finished. Starting sampling...\")\n",
    "    sampled_imgs = trainer.sample(num_images=4)\n",
    "    grid = torchvision.utils.make_grid(sampled_imgs, nrow=4, normalize=True)\n",
    "    torchvision.utils.save_image(grid, \"final_generated_sample.png\")\n",
    "    print(\"Saved final generated images to 'final_generated_sample.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aab04b",
   "metadata": {
    "papermill": {
     "duration": 2.593283,
     "end_time": "2025-09-02T18:21:01.709106",
     "exception": false,
     "start_time": "2025-09-02T18:20:59.115823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba6c42",
   "metadata": {
    "papermill": {
     "duration": 2.59992,
     "end_time": "2025-09-02T18:21:06.785259",
     "exception": false,
     "start_time": "2025-09-02T18:21:04.185339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 557,
     "sourceId": 1096,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 57490,
     "sourceId": 110680,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 471346,
     "sourceId": 883439,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 690564,
     "sourceId": 1229189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 757568,
     "sourceId": 1308548,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 886101,
     "sourceId": 1505106,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1285662,
     "sourceId": 2142690,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1834272,
     "sourceId": 3013107,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1280375,
     "sourceId": 3952819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1243366,
     "sourceId": 4077749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2458057,
     "sourceId": 4164627,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1441486,
     "sourceId": 4871142,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3066741,
     "sourceId": 5268557,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4212684,
     "sourceId": 7267543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4284904,
     "sourceId": 7374465,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4384844,
     "sourceId": 7544324,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4289.968612,
   "end_time": "2025-09-02T18:21:12.666446",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-02T17:09:42.697834",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

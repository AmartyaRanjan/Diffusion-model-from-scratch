{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1096,"sourceType":"datasetVersion","datasetId":557},{"sourceId":110680,"sourceType":"datasetVersion","datasetId":57490},{"sourceId":883439,"sourceType":"datasetVersion","datasetId":471346},{"sourceId":1229189,"sourceType":"datasetVersion","datasetId":690564},{"sourceId":1308548,"sourceType":"datasetVersion","datasetId":757568},{"sourceId":1505106,"sourceType":"datasetVersion","datasetId":886101},{"sourceId":2142690,"sourceType":"datasetVersion","datasetId":1285662},{"sourceId":3013107,"sourceType":"datasetVersion","datasetId":1834272},{"sourceId":3952819,"sourceType":"datasetVersion","datasetId":1280375},{"sourceId":4077749,"sourceType":"datasetVersion","datasetId":1243366},{"sourceId":4164627,"sourceType":"datasetVersion","datasetId":2458057},{"sourceId":4871142,"sourceType":"datasetVersion","datasetId":1441486},{"sourceId":5268557,"sourceType":"datasetVersion","datasetId":3066741},{"sourceId":7267543,"sourceType":"datasetVersion","datasetId":4212684},{"sourceId":7374465,"sourceType":"datasetVersion","datasetId":4284904},{"sourceId":7544324,"sourceType":"datasetVersion","datasetId":4384844}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# The default path for Kaggle input files\ninput_path = '/kaggle/input/'\n\n# List all items in the input directory and filter for folders\nfolders = [folder for folder in os.listdir(input_path) \n           if os.path.isdir(os.path.join(input_path, folder))]\n\n# Print the list of folders\nprint(folders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T17:22:54.935021Z","iopub.execute_input":"2025-09-01T17:22:54.935331Z","iopub.status.idle":"2025-09-01T17:22:54.944500Z","shell.execute_reply.started":"2025-09-01T17:22:54.935304Z","shell.execute_reply":"2025-09-01T17:22:54.943958Z"}},"outputs":[{"name":"stdout","text":"['postage-stamp-data-set', 'indian-paintings-dataset', 'india-famous-personalities-image-dataset', 'indian-actor-images-dataset', '200-bird-species-with-11788-images', 'indian-classical-musical-instruments', 'indian-currency-notes-classifier', 'indian-signboard-image-dataset', 'indian-currency-note-images-dataset-2020', 'indian-monuments-image-dataset', 'indian-dance-images', 'indian-cricketers-images', 'indian-food-images-dataset', 'bird-species-classification', 'snake-dataset-india', 'top-500-indian-cities']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nfrom tqdm import tqdm\n\n# --- Configuration ---\n# 1. Define the base path for Kaggle input\nBASE_INPUT_PATH = '/kaggle/input/'\n\n# 2. List of the source folders you want to combine\nSOURCE_FOLDERS = [\n    'india-famous-personalities-image-dataset', 'indian-classical-musical-instruments',\n     'indian-dance-images',\n    'indian-monuments-image-dataset', 'indian-currency-note-images-dataset-2020',\n    'postage-stamp-data-set',\n    'indian-currency-notes-classifier', 'indian-food-images-dataset', 'top-500-indian-cities', 'indian-paintings-dataset'\n]\n\n# 3. Define the destination for your new, combined dataset\n#    Kaggle allows you to write to the '/kaggle/working/' directory\nDESTINATION_PATH = '/kaggle/working/combined_indian_dataset/'\n\n# --- Main Script ---\nprint(f\"Starting the dataset combination process...\")\n\n# Create the destination folder if it doesn't exist\nos.makedirs(DESTINATION_PATH, exist_ok=True)\nprint(f\"Destination folder created at: {DESTINATION_PATH}\")\n\nimage_counter = 0\nimage_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n\n# Loop through each source folder\nfor folder_name in tqdm(SOURCE_FOLDERS, desc=\"Processing Folders\"):\n    source_path = os.path.join(BASE_INPUT_PATH, folder_name)\n    \n    # Check if the source directory actually exists\n    if not os.path.isdir(source_path):\n        print(f\"Warning: Folder '{folder_name}' not found. Skipping.\")\n        continue\n\n    # os.walk() recursively finds all files in all subdirectories\n    for dirpath, _, filenames in os.walk(source_path):\n        for filename in filenames:\n            # Check if the file is an image\n            if any(filename.lower().endswith(ext) for ext in image_extensions):\n                \n                # Construct the full path of the source image\n                source_file_path = os.path.join(dirpath, filename)\n                \n                # Get the original file extension (e.g., '.jpg')\n                original_extension = os.path.splitext(filename)[1]\n                \n                # Create a new, unique filename to avoid overwrites\n                new_filename = f\"image_{image_counter:06d}{original_extension}\"\n                destination_file_path = os.path.join(DESTINATION_PATH, new_filename)\n                \n                # Copy the file to the new destination\n                shutil.copy2(source_file_path, destination_file_path)\n                \n                # Increment the counter\n                image_counter += 1\n\nprint(\"\\n-------------------------------------------------\")\nprint(\"✅ Dataset combination complete!\")\nprint(f\"Total images copied: {image_counter}\")\nprint(f\"Your new dataset is ready at: {DESTINATION_PATH}\")\nprint(\"-------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T17:23:06.988063Z","iopub.execute_input":"2025-09-01T17:23:06.988898Z","iopub.status.idle":"2025-09-01T17:27:39.383257Z","shell.execute_reply.started":"2025-09-01T17:23:06.988873Z","shell.execute_reply":"2025-09-01T17:27:39.382364Z"}},"outputs":[{"name":"stdout","text":"Starting the dataset combination process...\nDestination folder created at: /kaggle/working/combined_indian_dataset/\n","output_type":"stream"},{"name":"stderr","text":"Processing Folders: 100%|██████████| 10/10 [04:32<00:00, 27.24s/it]","output_type":"stream"},{"name":"stdout","text":"\n-------------------------------------------------\n✅ Dataset combination complete!\nTotal images copied: 21813\nYour new dataset is ready at: /kaggle/working/combined_indian_dataset/\n-------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport shutil\nfrom tqdm import tqdm\n\n# The path to your combined dataset\nbase_path = '/kaggle/working/combined_indian_dataset/'\n\n# The path for the new sub-folder that ImageFolder expects\nnew_class_folder_path = os.path.join(base_path, 'images')\n\n# Create the new sub-folder\nprint(f\"Creating sub-folder at: {new_class_folder_path}\")\nos.makedirs(new_class_folder_path, exist_ok=True)\n\n# Find all files in the base directory\nfiles_to_move = [f for f in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, f))]\n\nprint(f\"Found {len(files_to_move)} images to move...\")\n\n# Move each file into the new sub-folder\nfor filename in tqdm(files_to_move, desc=\"Organizing files\"):\n    source = os.path.join(base_path, filename)\n    destination = os.path.join(new_class_folder_path, filename)\n    shutil.move(source, destination)\n\nprint(\"\\n✅ All files have been moved successfully!\")\nprint(\"You can now re-run your training script.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T17:27:39.384360Z","iopub.execute_input":"2025-09-01T17:27:39.385091Z","iopub.status.idle":"2025-09-01T17:27:40.111106Z","shell.execute_reply.started":"2025-09-01T17:27:39.385058Z","shell.execute_reply":"2025-09-01T17:27:40.110494Z"}},"outputs":[{"name":"stdout","text":"Creating sub-folder at: /kaggle/working/combined_indian_dataset/images\nFound 21813 images to move...\n","output_type":"stream"},{"name":"stderr","text":"Organizing files: 100%|██████████| 21813/21813 [00:00<00:00, 36069.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n✅ All files have been moved successfully!\nYou can now re-run your training script.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport math\nfrom tqdm import tqdm\nimport torchvision\nimport os\n\n# ======================================================================================\n# PART 1: THE VAE ARCHITECTURE (THE COMPRESSOR) - (No changes needed here)\n# ======================================================================================\nclass VAE(nn.Module):\n    def __init__(self, in_channels=3, latent_dim=128):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n        \n        modules = []\n        hidden_dims = [32, 64, 128, 256]\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels=h_dim,\n                              kernel_size=3, stride=2, padding=1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n        self.encoder = nn.Sequential(*modules)\n        \n        self.fc_mu = nn.Linear(hidden_dims[-1]*16, latent_dim)\n        self.fc_var = nn.Linear(hidden_dims[-1]*16, latent_dim)\n\n        modules = []\n        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 16)\n        hidden_dims.reverse()\n\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i],\n                                       hidden_dims[i + 1],\n                                       kernel_size=3, stride=2,\n                                       padding=1, output_padding=1),\n                    nn.BatchNorm2d(hidden_dims[i + 1]),\n                    nn.LeakyReLU())\n            )\n        self.decoder = nn.Sequential(*modules)\n        \n        self.final_layer = nn.Sequential(\n                            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3, stride=2,\n                                               padding=1, output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels=3,\n                                      kernel_size=3, padding=1),\n                            nn.Tanh())\n\n    def encode(self, x):\n        spatial_latent = self.encoder(x)\n        result = torch.flatten(spatial_latent, start_dim=1)\n        mu = self.fc_mu(result)\n        log_var = self.fc_var(result)\n        return mu, log_var, spatial_latent\n\n    def decode(self, z):\n        result = self.decoder_input(z)\n        result = result.view(-1, 256, 4, 4)\n        result = self.decoder(result)\n        result = self.final_layer(result)\n        return result\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        mu, log_var, _ = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        reconstruction = self.decode(z)\n        return reconstruction, mu, log_var\n\n# ======================================================================================\n# PART 2: THE U-NET ARCHITECTURE (THE DENOISER) - (No changes needed here)\n# ======================================================================================\nclass TimeEmbedding(nn.Module):\n    def __init__(self, n_channels: int):\n        super().__init__()\n        self.n_channels = n_channels\n    def forward(self, t: torch.Tensor):\n        half_dim = self.n_channels // 2\n        exponents = torch.arange(half_dim, device=t.device).float() / (half_dim - 1)\n        embeddings = torch.exp(-math.log(10000) * exponents)\n        embeddings = t[:, None] * embeddings[None, :]\n        return torch.cat([embeddings.sin(), embeddings.cos()], dim=-1)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, time_channels: int):\n        super().__init__()\n        self.norm1 = nn.GroupNorm(32, in_channels)\n        self.act1 = nn.SiLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.norm2 = nn.GroupNorm(32, out_channels)\n        self.act2 = nn.SiLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n        self.time_emb = nn.Linear(time_channels, out_channels)\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        h = self.conv1(self.act1(self.norm1(x)))\n        time_emb_proj = self.time_emb(self.act2(t))\n        h = h + time_emb_proj[:, :, None, None]\n        h = self.conv2(self.act2(self.norm2(h)))\n        return h + self.shortcut(x)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, n_channels: int):\n        super().__init__()\n        self.norm = nn.GroupNorm(32, n_channels)\n        self.qkv = nn.Conv2d(n_channels, n_channels * 3, kernel_size=1)\n        self.proj_out = nn.Conv2d(n_channels, n_channels, kernel_size=1)\n    def forward(self, x: torch.Tensor):\n        b, c, h, w = x.shape\n        h_ = self.norm(x)\n        qkv = self.qkv(h_)\n        q, k, v = qkv.chunk(3, dim=1)\n        q = q.view(b, c, h * w); k = k.view(b, c, h * w); v = v.view(b, c, h * w)\n        attn = torch.einsum('bci,bcj->bij', q, k) * (c ** -0.5)\n        attn = attn.softmax(dim=-1)\n        out = torch.einsum('bij,bcj->bci', attn, v)\n        out = out.view(b, c, h, w)\n        return x + self.proj_out(out)\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=256, out_channels=256, n_channels=320):\n        super().__init__()\n        time_emb_dim = n_channels * 4\n        self.time_embedding = TimeEmbedding(time_emb_dim)\n        self.inc = nn.Conv2d(in_channels, n_channels, kernel_size=3, padding=1)\n        self.down1_res = ResidualBlock(n_channels, n_channels, time_emb_dim)\n        self.down1_attn = AttentionBlock(n_channels)\n        self.down2_conv = nn.Conv2d(n_channels, n_channels * 2, kernel_size=3, stride=2, padding=1)\n        self.down2_res = ResidualBlock(n_channels * 2, n_channels * 2, time_emb_dim)\n        self.down2_attn = AttentionBlock(n_channels * 2)\n        self.bot_res1 = ResidualBlock(n_channels * 2, n_channels * 2, time_emb_dim)\n        self.bot_attn = AttentionBlock(n_channels * 2)\n        self.bot_res2 = ResidualBlock(n_channels * 2, n_channels * 2, time_emb_dim)\n        self.up1_res = ResidualBlock(n_channels * 4, n_channels * 2, time_emb_dim)\n        self.up1_attn = AttentionBlock(n_channels * 2)\n        self.up1_conv_transpose = nn.ConvTranspose2d(n_channels * 2, n_channels, kernel_size=2, stride=2)\n        self.up2_res = ResidualBlock(n_channels * 2, n_channels, time_emb_dim)\n        self.up2_attn = AttentionBlock(n_channels)\n        self.outc = nn.Sequential(nn.GroupNorm(32, n_channels), nn.SiLU(), nn.Conv2d(n_channels, out_channels, kernel_size=3, padding=1))\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        t_emb = self.time_embedding(t)\n        x1 = self.inc(x)\n        x2 = self.down1_res(x1, t_emb); x2 = self.down1_attn(x2)\n        x3_conv = self.down2_conv(x2)\n        x3 = self.down2_res(x3_conv, t_emb); x3 = self.down2_attn(x3)\n        x_bot = self.bot_res1(x3, t_emb); x_bot = self.bot_attn(x_bot); x_bot = self.bot_res2(x_bot, t_emb)\n        x = torch.cat([x_bot, x3], dim=1)\n        x = self.up1_res(x, t_emb); x = self.up1_attn(x); x = self.up1_conv_transpose(x)\n        x = torch.cat([x, x2], dim=1)\n        x = self.up2_res(x, t_emb); x = self.up2_attn(x)\n        return self.outc(x)\n\n# ======================================================================================\n# PART 3: THE DRIVER / CONTROLLER (THE LOGIC) - (Major changes here)\n# ======================================================================================\nclass DiffusionTrainer:\n    ### CHANGE: Added epochs to __init__ to configure the scheduler\n    def __init__(self, unet_model, vae_model, timesteps=1000, device='cpu', lr=1e-4, epochs=50):\n        self.device = device\n        self.unet_model = unet_model.to(device)\n        self.vae_model = vae_model.to(device)\n        self.timesteps = timesteps\n\n        for param in self.vae_model.parameters():\n            param.requires_grad = False\n        \n        self.betas = self._linear_beta_schedule(timesteps).to(device)\n        self.alphas = 1. - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n        \n        ### CHANGE: Switched to AdamW, a better optimizer\n        self.optimizer = torch.optim.AdamW(self.unet_model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n\n        ### CHANGE: Added a learning rate scheduler for better convergence\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=epochs)\n\n        ### CHANGE: Added a gradient scaler for automatic mixed precision (faster training)\n        self.scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n\n\n    def _linear_beta_schedule(self, timesteps):\n        beta_start = 0.0001\n        beta_end = 0.02\n        return torch.linspace(beta_start, beta_end, timesteps)\n\n    def _get_index_from_list(self, vals, t, x_shape):\n        batch_size = t.shape[0]\n        out = vals.gather(-1, t) \n        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n\n    def train_step(self, real_images):\n        self.optimizer.zero_grad()\n        \n        # Use autocast for mixed precision to speed up training on GPU\n        with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n            _, _, x_0 = self.vae_model.encode(real_images)\n            t = torch.randint(0, self.timesteps, (x_0.shape[0],), device=self.device).long()\n            noise = torch.randn_like(x_0)\n            \n            alphas_cumprod_t = self._get_index_from_list(self.alphas_cumprod, t, x_0.shape)\n            \n            noisy_latent = torch.sqrt(alphas_cumprod_t) * x_0 + torch.sqrt(1. - alphas_cumprod_t) * noise\n            predicted_noise = self.unet_model(noisy_latent, t)\n            loss = self.criterion(noise, predicted_noise)\n\n        ### CHANGE: Use the scaler for the backward pass\n        self.scaler.scale(loss).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        \n        return loss.item()\n\n    @torch.no_grad()\n    def sample(self, num_images=4):\n        latent_x = torch.randn((num_images, 256, 4, 4), device=self.device)\n        \n        for i in tqdm(reversed(range(0, self.timesteps)), desc=\"Sampling\", total=self.timesteps):\n            t = torch.full((num_images,), i, device=self.device, dtype=torch.long)\n            predicted_noise = self.unet_model(latent_x, t)\n            alpha_t = self._get_index_from_list(self.alphas, t, latent_x.shape)\n            alphas_cumprod_t = self._get_index_from_list(self.alphas_cumprod, t, latent_x.shape)\n            beta_t = self._get_index_from_list(self.betas, t, latent_x.shape)\n            noise_term = ((1 - alpha_t) / torch.sqrt(1 - alphas_cumprod_t)) * predicted_noise\n            latent_x = (1 / torch.sqrt(alpha_t)) * (latent_x - noise_term)\n            if i > 0:\n                z = torch.randn_like(latent_x)\n                latent_x += torch.sqrt(beta_t) * z\n\n        sampled_images = self.vae_model.decoder(latent_x)\n        sampled_images = self.vae_model.final_layer(sampled_images)\n        return sampled_images\n\n# ======================================================================================\n# PART 4: MAIN EXECUTION BLOCK (PUTTING IT ALL TOGETHER)\n# ======================================================================================\nif __name__ == '__main__':\n    # --- Configuration ---\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BATCH_SIZE = 8\n    IMG_SIZE = 64\n    ### CHANGE: Reduced epochs as requested. With faster training, 25 might be enough.\n    EPOCHS = 15\n    LEARNING_RATE = 2e-5 ### CHANGE: Slightly increased learning rate, good starting point with a scheduler\n    DATASET_PATH = \"/kaggle/working/combined_indian_dataset/\"\n\n    print(f\"Using device: {DEVICE}\")\n\n    # --- Data Loading and Transformations ---\n    transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    dataset = torchvision.datasets.ImageFolder(root=DATASET_PATH, transform=transforms)\n    \n    ### CHANGE: Reduced num_workers to prevent CPU bottleneck and added pin_memory for speed\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    \n    # --- Initialize Models ---\n    vae = VAE(in_channels=3, latent_dim=128)\n    # You should load your pre-trained VAE weights here if you have them\n    # vae.load_state_dict(torch.load('path/to/your/vae.pth'))\n    \n    VAE_ENCODER_OUTPUT_CHANNELS = 256 \n    unet = UNet(in_channels=VAE_ENCODER_OUTPUT_CHANNELS, out_channels=VAE_ENCODER_OUTPUT_CHANNELS)\n    \n    # --- Initialize the Driver/Controller ---\n    trainer = DiffusionTrainer(unet, vae, timesteps=1000, device=DEVICE, lr=LEARNING_RATE, epochs=EPOCHS)\n\n    # --- Training Loop ---\n    for epoch in range(EPOCHS):\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        for batch_idx, (real_images, _) in enumerate(progress_bar):\n            real_images = real_images.to(DEVICE)\n            loss = trainer.train_step(real_images)\n            total_loss += loss\n            progress_bar.set_postfix({\"Loss\": f\"{loss:.4f}\", \"LR\": f\"{trainer.scheduler.get_last_lr()[0]:.1e}\"})\n        \n        ### CHANGE: Step the scheduler at the end of each epoch\n        trainer.scheduler.step()\n        \n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n\n        ### CHANGE: Save a comprehensive checkpoint instead of just the weights\n        checkpoint = {\n            'epoch': epoch + 1,\n            'unet_state_dict': trainer.unet_model.state_dict(),\n            'optimizer_state_dict': trainer.optimizer.state_dict(),\n            'scheduler_state_dict': trainer.scheduler.state_dict(),\n            'loss': avg_loss,\n        }\n        torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pth\")\n        print(f\"Saved checkpoint to checkpoint_epoch_{epoch+1}.pth\")\n\n    # --- Sampling ---\n    print(\"Training finished. Starting sampling...\")\n    sampled_imgs = trainer.sample(num_images=4)\n    grid = torchvision.utils.make_grid(sampled_imgs, nrow=4, normalize=True)\n    torchvision.utils.save_image(grid, \"final_generated_sample.png\")\n    print(\"Saved final generated images to 'final_generated_sample.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T17:30:48.663066Z","iopub.execute_input":"2025-09-01T17:30:48.663722Z","execution_failed":"2025-09-01T18:20:11.187Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1740485352.py:195: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\nEpoch 1/15:   0%|          | 0/2725 [00:00<?, ?it/s]/tmp/ipykernel_36/1740485352.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\nEpoch 1/15:   0%|          | 11/2725 [00:05<10:31,  4.29it/s, Loss=1.0569, LR=2.0e-05] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1/15:  38%|███▊      | 1036/2725 [01:42<02:47, 10.10it/s, Loss=0.9161, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1/15: 100%|██████████| 2725/2725 [04:29<00:00, 10.10it/s, Loss=0.6903, LR=2.0e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 finished. Average Loss: 0.8496\nSaved checkpoint to checkpoint_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15:  13%|█▎        | 349/2725 [00:32<04:48,  8.24it/s, Loss=0.6569, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2/15:  17%|█▋        | 452/2725 [00:44<04:01,  9.42it/s, Loss=0.6551, LR=2.0e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2/15: 100%|██████████| 2725/2725 [04:24<00:00, 10.31it/s, Loss=0.6151, LR=2.0e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 finished. Average Loss: 0.6438\nSaved checkpoint to checkpoint_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15:   2%|▏         | 44/2725 [00:04<05:21,  8.34it/s, Loss=0.6186, LR=1.9e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3/15:  12%|█▏        | 323/2725 [00:32<03:02, 13.16it/s, Loss=0.6704, LR=1.9e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3/15: 100%|██████████| 2725/2725 [04:38<00:00,  9.77it/s, Loss=0.5662, LR=1.9e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 finished. Average Loss: 0.5494\nSaved checkpoint to checkpoint_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15:  15%|█▌        | 416/2725 [00:44<03:11, 12.03it/s, Loss=0.5566, LR=1.8e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4/15:  25%|██▍       | 678/2725 [01:11<04:15,  8.00it/s, Loss=0.5354, LR=1.8e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4/15: 100%|██████████| 2725/2725 [04:37<00:00,  9.83it/s, Loss=0.4354, LR=1.8e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 finished. Average Loss: 0.4884\nSaved checkpoint to checkpoint_epoch_4.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15:   1%|          | 30/2725 [00:03<05:42,  7.87it/s, Loss=0.4495, LR=1.7e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5/15: 100%|██████████| 2725/2725 [04:32<00:00,  9.99it/s, Loss=0.3771, LR=1.7e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 finished. Average Loss: 0.4381\nSaved checkpoint to checkpoint_epoch_5.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15:   6%|▌         | 159/2725 [00:16<05:55,  7.23it/s, Loss=0.4698, LR=1.5e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6/15:  80%|███████▉  | 2174/2725 [03:38<01:08,  7.99it/s, Loss=0.3407, LR=1.5e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6/15: 100%|██████████| 2725/2725 [04:31<00:00, 10.05it/s, Loss=0.5201, LR=1.5e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 finished. Average Loss: 0.4053\nSaved checkpoint to checkpoint_epoch_6.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15:   2%|▏         | 43/2725 [00:04<03:37, 12.35it/s, Loss=0.4416, LR=1.3e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7/15:  20%|██        | 556/2725 [00:55<03:12, 11.26it/s, Loss=0.4824, LR=1.3e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7/15: 100%|██████████| 2725/2725 [04:30<00:00, 10.07it/s, Loss=0.5190, LR=1.3e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 finished. Average Loss: 0.3809\nSaved checkpoint to checkpoint_epoch_7.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15:   6%|▌         | 154/2725 [00:16<03:03, 13.98it/s, Loss=0.3174, LR=1.1e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 8/15:  12%|█▏        | 330/2725 [00:34<05:28,  7.28it/s, Loss=0.4174, LR=1.1e-05]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 8/15: 100%|██████████| 2725/2725 [04:32<00:00,  9.98it/s, Loss=0.2555, LR=1.1e-05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 finished. Average Loss: 0.3570\nSaved checkpoint to checkpoint_epoch_8.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15:   3%|▎         | 85/2725 [00:08<03:25, 12.85it/s, Loss=0.4282, LR=9.0e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 9/15:   7%|▋         | 188/2725 [00:19<03:33, 11.86it/s, Loss=0.4004, LR=9.0e-06]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 9/15:  60%|██████    | 1639/2725 [02:46<01:48, 10.02it/s, Loss=0.2463, LR=9.0e-06]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}